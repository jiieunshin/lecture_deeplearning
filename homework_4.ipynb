{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "homework_4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpgXjbnKbuM"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQrmwO17xe4x"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import enum\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "from konlpy.tag import Okt # 한글 형태소 활용\n",
        "from konlpy.tag import Twitter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1yvQ3jKj9Y"
      },
      "source": [
        "def load_data(path):\n",
        "    # 판다스를 통해서 데이터를 불러온다.\n",
        "    data_df = pd.read_csv(path, header=0)\n",
        "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "    return question, answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYqt8nx1Kptg"
      },
      "source": [
        "def data_tokenizer(data):\n",
        "    # 토크나이징 해서 담을 배열 생성\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 위 필터와 같은 값들을 정규화 표현식을\n",
        "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    # 토그나이징과 정규표현식을 통해 만들어진\n",
        "    # 값들을 넘겨 준다.\n",
        "    return [word for word in words if word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0uO7r6xKsEY"
      },
      "source": [
        "# 한글 텍스트를 토크나이징 하기 위해 형태로소 분리하는 함수.\n",
        "# Konlpy에서 제공하는 Okt를 사용해 형태소 기준으로 텍스트 데이터를 토크나이징 한다. \n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        try :\n",
        "            morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "            result_data.append(morphlized_seq)\n",
        "        except Exception as exception :\n",
        "            print(\"오류 발생!!\")\n",
        "    return result_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJontl90KucL"
      },
      "source": [
        "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
        "    # 사전을 담을 배열 준비한다.\n",
        "    vocabulary_list = []\n",
        "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
        "    # 그 파일의 존재 유무를 확인한다.\n",
        "    if not os.path.exists(vocab_path):\n",
        "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
        "        # 데이터를 가지고 만들어야 한다.\n",
        "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
        "        # 데이터 파일의 존재 유무를 확인한다.\n",
        "        if (os.path.exists(path)):\n",
        "            # 데이터가 존재하니 판다스를 통해서\n",
        "            # 데이터를 불러오자\n",
        "            data_df = pd.read_csv(path, encoding='utf-8')\n",
        "            # 판다스의 데이터 프레임을 통해서\n",
        "            # 질문과 답에 대한 열을 가져 온다.\n",
        "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "            if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
        "                question = prepro_like_morphlized(question)\n",
        "                answer = prepro_like_morphlized(answer)\n",
        "            data = []\n",
        "            # 질문과 답변을 extend을\n",
        "            # 통해서 구조가 없는 배열로 만든다.\n",
        "            data.extend(question)\n",
        "            data.extend(answer)\n",
        "            # 토큰나이져 처리 하는 부분이다.\n",
        "            words = data_tokenizer(data)\n",
        "            # 공통적인 단어에 대해서는 모두\n",
        "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
        "            # set해주고 이것들을 리스트로 만들어 준다.\n",
        "            words = list(set(words))\n",
        "            # 데이터 없는 내용중에 MARKER를 사전에\n",
        "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
        "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
        "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
        "            # PAD = \"<PADDING>\"\n",
        "            # STD = \"<START>\"\n",
        "            # END = \"<END>\"\n",
        "            # UNK = \"<UNKNWON>\"\n",
        "            words[:0] = MARKER\n",
        "        # 사전을 리스트로 만들었으니 이 내용을\n",
        "        # 사전 파일을 만들어 넣는다.\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
        "            for word in words:\n",
        "                vocabulary_file.write(word + '\\n')\n",
        "\n",
        "    # 사전 파일이 존재하면 여기에서\n",
        "    # 그 파일을 불러서 배열에 넣어 준다.\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary_list.append(line.strip())\n",
        "\n",
        "    # 배열에 내용을 키와 값이 있는\n",
        "    # 딕셔너리 구조로 만든다.\n",
        "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return char2idx, idx2char, len(char2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY4AYjpTKwSq"
      },
      "source": [
        "def make_vocabulary(vocabulary_list):\n",
        "    # 리스트를 키가 단어이고 값이 인덱스인\n",
        "    # 딕셔너리를 만든다.\n",
        "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
        "    # 리스트를 키가 인덱스이고 값이 단어인\n",
        "    # 딕셔너리를 만든다.\n",
        "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
        "    # 두개의 딕셔너리를 넘겨 준다.\n",
        "    return char2idx, idx2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bPQKLSDK0jw"
      },
      "source": [
        "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다.)\n",
        "    sequences_input_index = []\n",
        "    # 하나의 인코딩 되는 문장의\n",
        "    # 길이를 가지고 있다.(누적된다.)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 인코딩 할때\n",
        "        # 가지고 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 문장을 스페이스 단위로\n",
        "        # 자르고 있다.\n",
        "        for word in sequence.split():\n",
        "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
        "            # 그 값을 가져와 sequence_index에 추가한다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.extend([dictionary[word]])\n",
        "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
        "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
        "            else:\n",
        "                sequence_index.extend([dictionary[UNK]])\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_input_index에 넣어 준다.\n",
        "        sequences_input_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과\n",
        "    # 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_input_index), sequences_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKgcUQ2TK5Su"
      },
      "source": [
        "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다)\n",
        "    sequences_output_index = []\n",
        "    # 하나의 디코딩 입력 되는 문장의\n",
        "    # 길이를 가지고 있다.(누적된다)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 디코딩 할때 가지고\n",
        "        # 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 디코딩 입력의 처음에는 START가 와야 하므로\n",
        "        # 그 값을 넣어 주고 시작한다.\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
        "        # 값인 인덱스를 넣어 준다.\n",
        "        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_output_index 넣어 준다.\n",
        "        sequences_output_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_output_index), sequences_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DekSOl9PJqiX"
      },
      "source": [
        "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다)\n",
        "    sequences_target_index = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
        "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
        "        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        # 그리고 END 토큰을 넣어 준다\n",
        "        if len(sequence_index) >= MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
        "        else:\n",
        "            sequence_index += [dictionary[END]]\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_target_index에 넣어 준다.\n",
        "        sequences_target_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_target_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHgC33OwyKjJ"
      },
      "source": [
        "# 시각화 함수\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ze_GPkvyern"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McDM3R4lyez7"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRNWDeYk2SRw"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXK6_bzm2Sb8"
      },
      "source": [
        "# pos/100002i/dim 값을 만드는 함수\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tFgHYg12Seq"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHjtZnQZ2Sj6"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    # 매우 작은 값을 넣어줌으로써 다음 레이어인 softmax에서 무시할 수 있도록 함 (softmax: 매우작은 음수값 -> 0에 수렴하는 값을 가짐)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJw2TpNk2Smv"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        # 나머지가 발생하면 에러가 나게끔 함\n",
        "        assert self.d_model % self.num_heads == 0 \n",
        "\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    # key, query, value에 대한 벡터를 헤드 수만큼 분리할 수 있게 하는 함수 [batch, sequence, feature] -> [batch, head, sequence, feature]\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  # 피처 차원을 헤드 수 만틈 분리 [batch, sequence, head, feature]\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # 시퀀스, 헤드 자리를 바꿈: [batch, head, sequence, feature]\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3XtLTeH2SpB"
      },
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ldkFyv32Srx"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # self attention (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHKMQteW2uzF"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(**kargs)\n",
        "        self.mha2 = MultiHeadAttention(**kargs)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtJso4PL4SYB"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoomAgK02u2K"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3G5ZPUV2u5Q"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Transformer, self).__init__(name=kargs['model_name'])\n",
        "        self.end_token_idx = kargs['end_token_idx']\n",
        "        \n",
        "        self.encoder = Encoder(**kargs)\n",
        "        self.decoder = Decoder(**kargs)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
        "\n",
        "    def call(self, x):\n",
        "        inp, tar = x\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, _ = self.decoder(\n",
        "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    def inference(self, x):\n",
        "        inp = x\n",
        "        tar = tf.expand_dims([STD_INDEX], 0)\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)        \n",
        "        enc_output = self.encoder(inp, enc_padding_mask)\n",
        "        \n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "            final_output = self.final_layer(dec_output)\n",
        "            outputs = tf.argmax(final_output, -1).numpy()\n",
        "            pred_token = outputs[0][-1]\n",
        "            if pred_token == self.end_token_idx:\n",
        "                break\n",
        "            predict_tokens.append(pred_token)\n",
        "            tar = tf.expand_dims([STD_INDEX] + predict_tokens, 0)\n",
        "            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "            \n",
        "        return predict_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBZx0EPh2u79"
      },
      "source": [
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CzttoIBa0Nx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_kkY3kZPjhh"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/시립대_딥러닝/과제')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAyTFTHKhTa"
      },
      "source": [
        "FILTERS = \"([~.,!?\\\"':;)(])\" \n",
        "PAD = \"<PAD>\"\n",
        "STD = \"<SOS>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)   # ~.,!?\\\"':;)( 들과 매치되는지 확인하기 위함\n",
        "\n",
        "MAX_SEQUENCE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Dg0r1Tl8At"
      },
      "source": [
        "# PATH = 'data_in/ChatBotData_csv2.csv' \n",
        "PATH = 'data_in/BlueHouse_okt_100_03.csv'\n",
        "VOCAB_PATH = 'data_in/vocabulary.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O0TSvUrl8Zt"
      },
      "source": [
        "inputs, outputs = load_data(PATH)\n",
        "char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6hvuAQzvyVq"
      },
      "source": [
        "char2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnXT84eFmsd-"
      },
      "source": [
        "index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=True)\n",
        "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=True)\n",
        "index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMlElBevl8ld"
      },
      "source": [
        "data_configs = {}\n",
        "data_configs['char2idx'] = char2idx\n",
        "data_configs['idx2char'] = idx2char\n",
        "data_configs['vocab_size'] = vocab_size\n",
        "data_configs['pad_symbol'] = PAD\n",
        "data_configs['std_symbol'] = STD\n",
        "data_configs['end_symbol'] = END\n",
        "data_configs['unk_symbol'] = UNK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoBEyexUmBqr"
      },
      "source": [
        "# 경로설정\n",
        "DATA_IN_PATH = './data_in/'\n",
        "DATA_OUT_PATH = './data_out/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spo5ByROmCNV"
      },
      "source": [
        "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG-GWF51mCQV"
      },
      "source": [
        "char2idx\n",
        "idx2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBMvjM3smCT_"
      },
      "source": [
        "# 랜덤시드 고정\n",
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thrVbMqumLF9"
      },
      "source": [
        "# 파일로드\n",
        "index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n",
        "index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'rb'))\n",
        "index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS , 'rb'))\n",
        "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydMmdThlmLJA"
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "model_name = 'transformer'\n",
        "vocab_size = prepro_configs['vocab_size']\n",
        "BATCH_SIZE = 2\n",
        "MAX_SEQUENCE = 50\n",
        "EPOCHS = 2\n",
        "VALID_SPLIT = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUtpv5f7mLMz"
      },
      "source": [
        "kargs = {'model_name': model_name,\n",
        "         'num_layers': 2,\n",
        "         'd_model': 512, # 512/ 8 = 64 \n",
        "         'num_heads': 8,\n",
        "         'dff': 2048, # 512 * 4\n",
        "         'input_vocab_size': vocab_size,\n",
        "         'target_vocab_size': vocab_size,\n",
        "         'maximum_position_encoding': MAX_SEQUENCE,\n",
        "         'end_token_idx': char2idx[end_index],\n",
        "         'rate': 0.1\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5wCM54KmLPP"
      },
      "source": [
        "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)        \n",
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73vEij4mLS2"
      },
      "source": [
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QRuWZOUH5Gc"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNURAxr22u-y"
      },
      "source": [
        "model = Transformer(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vp5VB1a29w1"
      },
      "source": [
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n",
        "\n",
        "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0RW3yRx29z1"
      },
      "source": [
        "history = model.fit([index_inputs, index_outputs], index_targets, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoSJUbEQ293c"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93dqNbKObEnI"
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVhakXTA2959"
      },
      "source": [
        "DATA_OUT_PATH = './data_out/'\n",
        "SAVE_FILE_NM = 'weights.h5'\n",
        "\n",
        "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugjl1XGq298u"
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToRWIvFX2-BH"
      },
      "source": [
        "text = \"양성평등\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "print(test_index_inputs)\n",
        "print(model.inference(test_index_inputs))\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}